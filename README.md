# ğŸ“ˆ Quantitative Analytics Dashboard  
A complete real-time analytics system built for the **Gemscap Quant Developer Evaluation Assignment**, demonstrating ingestion â†’ storage â†’ analytics â†’ visualization â†’ alerts â†’ backtesting.

This project is designed as a **modular, scalable quant research tool**, suitable for statistical arbitrage, cross-asset modelling, and real-time market monitoring.

---

# ğŸš€ 1. Project Objectives

This application demonstrates:

- Real-time **tick ingestion** through WebSocket  
- **Storage** into a persistent database  
- **Resampling** to OHLC (1s, 1m, 5m)  
- **Pair-trading analytics**  
- **Dynamic hedge ratio** via Kalman Filter (advanced)  
- **Interactive dashboard** with multiple visualizations  
- **Alerts and data export**  
- **Backtesting** for mean-reversion strategy  
- **Extensible ML feature generation layer**  

It satisfies **all mandatory** expectations and multiple **advanced extensions**.

---

# ğŸ— 2. Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Binance WebSocket Stream     â”‚
â”‚  (tick-level price & volume)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
         Realtime ticks
                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Ingestion Pipeline        â”‚
â”‚  - Parses WS messages         â”‚
â”‚  - Validates/normalizes       â”‚
â”‚  - Stores into PostgreSQL     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
       Stored tick table
                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Resampling Engine        â”‚
â”‚  - 1s, 1m, 5m OHLC            â”‚
â”‚  - Volume aggregation         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
        Clean OHLC dataset
                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Analytics Engine          â”‚
â”‚  - OLS Regression Î², RÂ²       â”‚
â”‚  - Kalman Filter Î²(t)         â”‚
â”‚  - Spread, Z-score            â”‚
â”‚  - ADF mean-reversion test    â”‚
â”‚  - Rolling correlation        â”‚
â”‚  - ML feature table           â”‚
â”‚  - Backtesting engine         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
             Results
                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Streamlit Frontend UI       â”‚
â”‚  - Candles, spreads, z-score  â”‚
â”‚  - Data explorer & stats      â”‚
â”‚  - Alerts                     â”‚
â”‚  - CSV export                 â”‚
â”‚  - Controls for all params    â”‚
â”‚  - Live auto-refresh          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# ğŸ”Œ 3. Data Flow Summary

1ï¸âƒ£ **Tick ingestion** from WebSocket  
2ï¸âƒ£ **Database storage** (PostgreSQL)  
3ï¸âƒ£ **Sampling** â†’ 1s/1m/5m OHLC  
4ï¸âƒ£ **Analytics computation**  
5ï¸âƒ£ **Live dashboard update** (configurable refresh interval)  
6ï¸âƒ£ **Alert scanning**  
7ï¸âƒ£ **Exports and ML pipeline**  

---

# ğŸ’¡ 4. Design Philosophy

This project is intentionally structured with:

### âœ” **Loose coupling**
- Ingestion â†’ Storage â†’ Analytics â†’ UI  
are completely modular.

### âœ” **Extensibility**
New analytics (e.g., co-integration, Hurst exponent, PCA factors) can be added without modifying ingestion.

### âœ” **Scalability readiness**
- Database layer can move from PostgreSQL â†’ TimescaleDB â†’ ClickHouse  
- Ingestion can switch from WebSocket â†’ Kafka â†’ FIX feed  
- UI can migrate to Dash/React without backend rewrite  

### âœ” **Real-time system design**
- Streamlit auto-refresh simulates live monitoring  
- Tick-to-analytics latency remains low (< 500ms achievable)

### âœ” **Simplicity in code**
Readable, documented, beginner-friendly while achieving professional modularity.

---

# ğŸ§  5. Why These Design Choices?

### PostgreSQL  
Easy to query resampling, flexible schema, reliable ACID store.

### Streamlit  
Best Python-native UI framework for fast prototyping.

### Plotly  
High interactivity (pan, zoom, hover) + financial charting quality.

### Kalman Filter  
Reflects real hedge ratio dynamics â€” useful in statistical arbitrage.

### Modular files
- `analytics.py` â†’ maths & stats  
- `ingest_websocket.py` â†’ realtime ingestion  
- `dashboard_streamlit.py` â†’ UI  
- `database_utils.py` â†’ storage logic  

This modular architecture mirrors what modern quant teams use.

---

# ğŸ“Š 6. Implemented Analytics (ALL Required)

### âœ” OLS Hedge Ratio  
### âœ” Kalman Filter Hedge Ratio (advanced)  
### âœ” Spread  
### âœ” Z-score  
### âœ” ADF test  
### âœ” Rolling correlation  
### âœ” Price charts & volume  
### âœ” Technical indicators (SMA, EMA, VWAP, Bollinger Bands)  
### âœ” ML feature table (advanced)  

---

# ğŸ”¥ 7. Live Analytics 

This is now explicitly documented:

- Dashboard auto-refreshes based on `STREAMLIT_REFRESH_MS`  
- Z-score & spread recompute automatically  
- Rolling correlation updates  
- Alerts trigger without page reload  

# Analytics Methodology

### **Hedge Ratio (OLS)**  
```
Y = Î²X + Îµ
```

- Î² = hedge ratio  
- RÂ² = explanatory strength  

---

### **Hedge Ratio (Kalman Filter)**
Dynamic Î²(t):

Useful for regime shifts.

---

### **Spread**
```
spread = y - Î²x
```

---

### **Z-Score**
```
z = (spread - mean) / std
```

---

### **ADF Test**
Checks if spread is mean-reverting.

---

### **Rolling Correlation**
Measures time-varying correlation between X and Y.

---

# ğŸ§ª 8. Backtesting Module

Implements the assignmentâ€™s required:

### âœ” Mean-reversion (|Z| > 2 entry, |Z| < 0 exit)  
### âœ” Stop-loss  
### âœ” Equity curve  
### âœ” PnL, Max Drawdown, Sharpe  
### âœ” Trades table  


### **Entry**
- Long spread if Z < â€“entry_z  
- Short spread if Z > entry_z  

### **Exit**
- |Z| < exit_z  
- OR stop-loss  

### **Outputs**
- Total PnL  
- Max Drawdown  
- Sharpe Ratio  
- Number of trades  
- Equity curve 

---

# ğŸš¨ 9. Alerting System

Supports:

- Z-score > threshold  
- Z-score < threshold  
- Spread > threshold  
- Spread < threshold  

Alerts update live with each refresh cycle.

---

# ğŸ“ 10. Data Uploads 

Users may upload their own OHLC CSV:

- Must contain `{timestamp, open, high, low, close, volume}`  
- Stored in database  
- Fully integrated into analytics  

---

# ğŸ“¤ 11. Data Export (Required)

Supports:

- OHLC download  
- Spread CSV  
- Z-score CSV  
- ML feature table CSV  

---

# ğŸ“˜ 12. Setup Instructions

### **1. Clone repo**
```bash
git clone <https://github.com/akashmestha/Quant.git>
cd quant-dashboard
```

### **2. Create virtual environment**
```bash
python -m venv venv
source venv/bin/activate
```

### **3. Install dependencies**
```bash
pip install -r requirements.txt
```

### **4. Create `.env`**
```
DATABASE_URL=postgresql://user:password@localhost:5432/quantdb
STREAMLIT_REFRESH_MS=1000
```

### **5. Run ingestion**
```bash
python ingest_websocket.py
```

### **6. Run dashboard**
```bash
streamlit run dashboard_streamlit.py
```


---



# ğŸ¤– 13. ChatGPT Usage Transparency  


ChatGPT was used for:

- Designing architecture  
- Debugging Streamlit  
- Improving modularity  
- Implementing Kalman filter logic  
- Writing documentation  
- Optimizing code readability  

All final code was manually reviewed and validated.
---

# ğŸ 14. Conclusion

This project demonstrates:

- End-to-end quant development  
- Real-time ingestion  
- Advanced analytics  
- Interactive visualizations  
- Extensible modular structure  